{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fcf666e1-1182-4821-9650-b81614303055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1272ccd0-5af2-4f1e-a83f-20ca29971613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14034 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        r\"E:\\Zip files\\intel image classification\\seg_train\\seg_train\",\n",
    "        batch_size=32,\n",
    "        target_size=(64, 64),\n",
    "        class_mode = \"categorical\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f29edcee-128d-4310-9cda-66815188a524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    r\"E:\\Zip files\\intel image classification\\seg_test\\seg_test\",\n",
    "    batch_size=32,\n",
    "    target_size=(64, 64),\n",
    "    class_mode = \"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "52a99e70-098c-41d4-b44e-f196e837fba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'buildings': 0,\n",
       " 'forest': 1,\n",
       " 'glacier': 2,\n",
       " 'mountain': 3,\n",
       " 'sea': 4,\n",
       " 'street': 5}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = training_set.class_indices\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1e797629-e2c6-49b6-8924-77f0b8fd2330",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a3c2a4b4-348f-4f7b-b46d-fef5e65cf716",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters = 32,\n",
    "    kernel_size = 3,\n",
    "    padding='valid',\n",
    "    activation=\"relu\",\n",
    "    input_shape = [64, 64, 3]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3823677a-6e00-4d98-9f8b-12dff6532622",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=2,\n",
    "    strides=2\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e6cf2e96-0dea-42a4-9924-7414db9dd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dropout(\n",
    "    rate = 0.3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d53091c4-c153-4210-bda4-391c3dce834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters = 32,\n",
    "    kernel_size = 3,\n",
    "    activation=\"relu\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "74a1c8e3-d899-4a55-afb9-87efd341a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=2,\n",
    "    strides=2\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "aacf75ba-1a28-4d08-b105-20d29c231c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dropout(\n",
    "    rate = 0.4\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e733b1e6-1c18-4d92-b1d4-ac17dbf8acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Conv2D(\n",
    "    filters = 64,\n",
    "    kernel_size = 3,\n",
    "    activation=\"relu\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "382f57b8-605c-45d2-a58e-e7d0f450c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=2,\n",
    "    strides=2\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9c30f0aa-ce7f-4084-aad8-11b33560ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dropout(\n",
    "    rate = 0.5\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0b94f9cf-bacd-44fd-9414-0a02a7729faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dabcfc78-0fba-439b-9574-de9eacf6e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(\n",
    "    units = 128,\n",
    "    activation=\"relu\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "994690e4-1691-483e-ba72-7fabc687421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dropout(\n",
    "    rate = 0.6\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a656d345-d7e4-45e7-b0ce-0b6f9760c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(\n",
    "    units = 6,\n",
    "    activation=\"sigmoid\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a4105d5f-889b-4c81-96cd-7797142b090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf625130-6885-4198-b7ec-9934b3691d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "439/439 [==============================] - 36s 80ms/step - loss: 0.3620 - accuracy: 0.4620 - val_loss: 0.3240 - val_accuracy: 0.5600\n",
      "Epoch 2/20\n",
      "439/439 [==============================] - 35s 80ms/step - loss: 0.2915 - accuracy: 0.6022 - val_loss: 0.2723 - val_accuracy: 0.6390\n",
      "Epoch 3/20\n",
      "439/439 [==============================] - 36s 83ms/step - loss: 0.2656 - accuracy: 0.6585 - val_loss: 0.2296 - val_accuracy: 0.7100\n",
      "Epoch 4/20\n",
      "439/439 [==============================] - 38s 86ms/step - loss: 0.2479 - accuracy: 0.6837 - val_loss: 0.2310 - val_accuracy: 0.6983\n",
      "Epoch 5/20\n",
      "439/439 [==============================] - 36s 81ms/step - loss: 0.2341 - accuracy: 0.7096 - val_loss: 0.1950 - val_accuracy: 0.7670\n",
      "Epoch 6/20\n",
      "439/439 [==============================] - 40s 91ms/step - loss: 0.2241 - accuracy: 0.7254 - val_loss: 0.2054 - val_accuracy: 0.7420\n",
      "Epoch 7/20\n",
      "439/439 [==============================] - 38s 88ms/step - loss: 0.2148 - accuracy: 0.7398 - val_loss: 0.1774 - val_accuracy: 0.7880\n",
      "Epoch 8/20\n",
      "439/439 [==============================] - 40s 91ms/step - loss: 0.2102 - accuracy: 0.7471 - val_loss: 0.2009 - val_accuracy: 0.7520\n",
      "Epoch 9/20\n",
      "439/439 [==============================] - 57s 131ms/step - loss: 0.2058 - accuracy: 0.7557 - val_loss: 0.1843 - val_accuracy: 0.7723\n",
      "Epoch 10/20\n",
      "439/439 [==============================] - 41s 93ms/step - loss: 0.2015 - accuracy: 0.7616 - val_loss: 0.1817 - val_accuracy: 0.7877\n",
      "Epoch 11/20\n",
      "439/439 [==============================] - 37s 84ms/step - loss: 0.1979 - accuracy: 0.7634 - val_loss: 0.1723 - val_accuracy: 0.8027\n",
      "Epoch 12/20\n",
      "439/439 [==============================] - 37s 85ms/step - loss: 0.1887 - accuracy: 0.7764 - val_loss: 0.1863 - val_accuracy: 0.7797\n",
      "Epoch 13/20\n",
      "439/439 [==============================] - 35s 81ms/step - loss: 0.1893 - accuracy: 0.7779 - val_loss: 0.1574 - val_accuracy: 0.8170\n",
      "Epoch 14/20\n",
      "439/439 [==============================] - 38s 87ms/step - loss: 0.1856 - accuracy: 0.7820 - val_loss: 0.1857 - val_accuracy: 0.7737\n",
      "Epoch 15/20\n",
      "439/439 [==============================] - 38s 86ms/step - loss: 0.1835 - accuracy: 0.7882 - val_loss: 0.1649 - val_accuracy: 0.8043\n",
      "Epoch 16/20\n",
      "439/439 [==============================] - 35s 80ms/step - loss: 0.1806 - accuracy: 0.7888 - val_loss: 0.1673 - val_accuracy: 0.8023\n",
      "Epoch 17/20\n",
      "439/439 [==============================] - 35s 80ms/step - loss: 0.1835 - accuracy: 0.7838 - val_loss: 0.2076 - val_accuracy: 0.7353\n",
      "Epoch 18/20\n",
      "439/439 [==============================] - 35s 80ms/step - loss: 0.1780 - accuracy: 0.7931 - val_loss: 0.1628 - val_accuracy: 0.8037\n",
      "Epoch 19/20\n",
      "439/439 [==============================] - 38s 87ms/step - loss: 0.1781 - accuracy: 0.7919 - val_loss: 0.1780 - val_accuracy: 0.7820\n",
      "Epoch 20/20\n",
      "137/439 [========>.....................] - ETA: 22s - loss: 0.1777 - accuracy: 0.7993"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=training_set,\n",
    "    epochs=20,\n",
    "    validation_data=test_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "94a0b222-add1-442e-8311-fdc7b1f4a975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64, 64, 3)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[[0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "test_image =image.load_img(r\"E:\\Zip files\\intel image classification\\seg_pred\\seg_pred\\761.jpg\", target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "print(test_image.shape)\n",
    "result = model.predict(test_image)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a2fbe16f-8e08-427c-879a-44a2683c6b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glacier\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in (result):\n",
    "    for j in enumerate(i):\n",
    "        if j[1] == 1.0:\n",
    "            count = j[0]\n",
    "for i, j in val.items():\n",
    "    if j == count:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bf920992-73de-43f3-9b81-e02dccea62c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468299d-a298-4c1f-b2a5-5a0aeac7ac42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
